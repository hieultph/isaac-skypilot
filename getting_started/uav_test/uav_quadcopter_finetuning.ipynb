{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4073a809",
   "metadata": {},
   "source": [
    "# UAV Quadcopter Finetuning Tutorial\n",
    "\n",
    "This notebook demonstrates how to adapt the GR00T humanoid robot VLA model for UAV quadcopter control.\n",
    "\n",
    "## State Space (13D):\n",
    "- **position**: x, y, z (3 dims)\n",
    "- **orientation**: roll, pitch, yaw (3 dims)\n",
    "- **velocity**: vx, vy, vz (3 dims)\n",
    "- **battery**: battery level (1 dim)\n",
    "- **gps**: lat, lon, alt (3 dims)\n",
    "\n",
    "## Action Space (9D):\n",
    "- **flight_control**: throttle, roll, pitch, yaw (4 dims)\n",
    "- **velocity_command**: vx, vy, vz (3 dims)\n",
    "- **gimbal**: gimbal_pitch, gimbal_yaw (2 dims)\n",
    "\n",
    "The key insight is to leverage the pretrained VLM part and only retrain the diffusion model for UAV action generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "from gr00t.data.dataset import ModalityConfig\n",
    "from gr00t.data.embodiment_tags import EmbodimentTag\n",
    "from gr00t.experiment.data_config import UAVQuadcopterDataConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e6f3f",
   "metadata": {},
   "source": [
    "## Step 1: Define UAV Modality Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd401de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UAV-specific modality configurations\n",
    "video_modality = ModalityConfig(\n",
    "    delta_indices=[0],\n",
    "    modality_keys=[\"video.front_camera\", \"video.gimbal_camera\"],\n",
    ")\n",
    "\n",
    "state_modality = ModalityConfig(\n",
    "    delta_indices=[0],\n",
    "    modality_keys=[\n",
    "        \"state.position\",      # x, y, z\n",
    "        \"state.orientation\",   # roll, pitch, yaw\n",
    "        \"state.velocity\",      # vx, vy, vz\n",
    "        \"state.battery\",       # battery level\n",
    "        \"state.gps\",          # lat, lon, alt\n",
    "    ],\n",
    ")\n",
    "\n",
    "action_modality = ModalityConfig(\n",
    "    delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    modality_keys=[\n",
    "        \"action.flight_control\",  # throttle, roll, pitch, yaw\n",
    "        \"action.velocity_command\", # vx, vy, vz\n",
    "        \"action.gimbal\",          # gimbal_pitch, gimbal_yaw\n",
    "    ],\n",
    ")\n",
    "\n",
    "language_modality = ModalityConfig(\n",
    "    delta_indices=[0],\n",
    "    modality_keys=[\"annotation.human.task_description\"],\n",
    ")\n",
    "\n",
    "modality_configs = {\n",
    "    \"video\": video_modality,\n",
    "    \"state\": state_modality,\n",
    "    \"action\": action_modality,\n",
    "    \"language\": language_modality,\n",
    "}\n",
    "\n",
    "print(\"UAV Modality configurations created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef39dd4",
   "metadata": {},
   "source": [
    "## Step 2: Configure UAV-specific Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.transform.base import ComposedModalityTransform\n",
    "from gr00t.data.transform import VideoToTensor, VideoResize, VideoColorJitter, VideoToNumpy\n",
    "from gr00t.data.transform.state_action import StateActionToTensor, StateActionTransform\n",
    "from gr00t.data.transform.concat import ConcatTransform\n",
    "from gr00t.model.transforms import GR00TTransform\n",
    "\n",
    "transforms = [\n",
    "    # Video transforms optimized for aerial footage\n",
    "    VideoToTensor(apply_to=video_modality.modality_keys),\n",
    "    VideoResize(\n",
    "        apply_to=video_modality.modality_keys,\n",
    "        height=224,\n",
    "        width=224,\n",
    "        antialias=True,\n",
    "    ),\n",
    "    VideoColorJitter(\n",
    "        apply_to=video_modality.modality_keys, \n",
    "        brightness=0.3, \n",
    "        contrast=0.4, \n",
    "        saturation=0.5, \n",
    "        hue=0.08, \n",
    "        backend=\"torchvision\"\n",
    "    ),\n",
    "    VideoToNumpy(apply_to=video_modality.modality_keys),\n",
    "\n",
    "    # State transforms for UAV telemetry\n",
    "    StateActionToTensor(apply_to=state_modality.modality_keys),\n",
    "    StateActionTransform(\n",
    "        apply_to=state_modality.modality_keys, \n",
    "        normalization_modes={\n",
    "            \"state.position\": \"min_max\",      # Normalize position coordinates\n",
    "            \"state.orientation\": \"min_max\",   # Normalize Euler angles\n",
    "            \"state.velocity\": \"min_max\",      # Normalize velocity vectors\n",
    "            \"state.battery\": \"min_max\",       # Normalize battery percentage\n",
    "            \"state.gps\": \"min_max\",          # Normalize GPS coordinates\n",
    "        },\n",
    "        target_rotations={\n",
    "            \"state.orientation\": \"euler_angles\",  # Use Euler angles for UAV orientation\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # Action transforms for UAV control\n",
    "    StateActionToTensor(apply_to=action_modality.modality_keys),\n",
    "    StateActionTransform(\n",
    "        apply_to=action_modality.modality_keys, \n",
    "        normalization_modes={\n",
    "            \"action.flight_control\": \"min_max\",   # Normalize flight controls\n",
    "            \"action.velocity_command\": \"min_max\", # Normalize velocity commands  \n",
    "            \"action.gimbal\": \"min_max\",           # Normalize gimbal controls\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # Concatenation transform\n",
    "    ConcatTransform(\n",
    "        video_concat_order=video_modality.modality_keys,\n",
    "        state_concat_order=state_modality.modality_keys,\n",
    "        action_concat_order=action_modality.modality_keys,\n",
    "    ),\n",
    "    \n",
    "    # GR00T-specific transform for UAV\n",
    "    GR00TTransform(\n",
    "        state_horizon=len(state_modality.delta_indices),\n",
    "        action_horizon=len(action_modality.delta_indices),\n",
    "        max_state_dim=64,  # Accommodate 13D state space\n",
    "        max_action_dim=32, # Accommodate 9D action space\n",
    "    ),\n",
    "]\n",
    "\n",
    "composed_transforms = ComposedModalityTransform(transforms=transforms)\n",
    "print(\"UAV-specific transforms configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94aa87e",
   "metadata": {},
   "source": [
    "## Step 3: Load UAV Dataset (Example with demo data structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81158197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths for UAV dataset\n",
    "dataset_path = \"./demo_data/uav.Landing\"  # Replace with your UAV dataset path\n",
    "embodiment_tag = EmbodimentTag.UAV_QUADCOPTER\n",
    "\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "print(f\"Embodiment tag: {embodiment_tag}\")\n",
    "\n",
    "# Note: You would need to prepare your UAV dataset in LeRobot format\n",
    "# with the modality.json file copied to meta/modality.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f2d0cc",
   "metadata": {},
   "source": [
    "## Step 4: Initialize UAV Dataset\n",
    "\n",
    "**Note**: This cell will work once you have actual UAV data prepared in LeRobot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79493221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when you have actual UAV data\n",
    "# uav_dataset = LeRobotSingleDataset(\n",
    "#     dataset_path=dataset_path,\n",
    "#     modality_configs=modality_configs,\n",
    "#     embodiment_tag=embodiment_tag,\n",
    "#     transforms=composed_transforms,\n",
    "#     video_backend=\"torchvision_av\",\n",
    "# )\n",
    "\n",
    "# print(f\"Initialized UAV dataset with {len(uav_dataset)} episodes\")\n",
    "# print(f\"Sample keys: {list(uav_dataset[0].keys())}\")\n",
    "\n",
    "print(\"Dataset initialization ready - add your UAV data to proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fee3ad",
   "metadata": {},
   "source": [
    "## Step 5: Visualize UAV State and Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example UAV state visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Mock data for visualization\n",
    "time_steps = np.arange(0, 100)\n",
    "position = np.random.randn(100, 3) * 10  # x, y, z\n",
    "orientation = np.random.randn(100, 3) * 0.5  # roll, pitch, yaw\n",
    "velocity = np.random.randn(100, 3) * 5  # vx, vy, vz\n",
    "battery = 100 - time_steps * 0.5  # decreasing battery\n",
    "flight_control = np.random.randn(100, 4) * 0.5  # throttle, roll, pitch, yaw\n",
    "gimbal = np.random.randn(100, 2) * 0.3  # gimbal_pitch, gimbal_yaw\n",
    "\n",
    "# Plot state space\n",
    "axes[0, 0].plot(time_steps, position)\n",
    "axes[0, 0].set_title('Position (x, y, z)')\n",
    "axes[0, 0].legend(['x', 'y', 'z'])\n",
    "\n",
    "axes[0, 1].plot(time_steps, orientation)\n",
    "axes[0, 1].set_title('Orientation (roll, pitch, yaw)')\n",
    "axes[0, 1].legend(['roll', 'pitch', 'yaw'])\n",
    "\n",
    "axes[0, 2].plot(time_steps, velocity)\n",
    "axes[0, 2].set_title('Velocity (vx, vy, vz)')\n",
    "axes[0, 2].legend(['vx', 'vy', 'vz'])\n",
    "\n",
    "# Plot action space\n",
    "axes[1, 0].plot(time_steps, flight_control)\n",
    "axes[1, 0].set_title('Flight Control (throttle, roll, pitch, yaw)')\n",
    "axes[1, 0].legend(['throttle', 'roll', 'pitch', 'yaw'])\n",
    "\n",
    "axes[1, 1].plot(time_steps, battery)\n",
    "axes[1, 1].set_title('Battery Level')\n",
    "\n",
    "axes[1, 2].plot(time_steps, gimbal)\n",
    "axes[1, 2].set_title('Gimbal Control (pitch, yaw)')\n",
    "axes[1, 2].legend(['pitch', 'yaw'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nUAV State/Action Space Dimensions:\")\n",
    "print(f\"State Space: 13D (position: 3, orientation: 3, velocity: 3, battery: 1, gps: 3)\")\n",
    "print(f\"Action Space: 9D (flight_control: 4, velocity_command: 3, gimbal: 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0fba3",
   "metadata": {},
   "source": [
    "## Step 6: Fine-tuning Configuration\n",
    "\n",
    "Key differences for UAV adaptation:\n",
    "1. **Leverage VLM**: Keep the visual language model frozen to retain visual understanding\n",
    "2. **Retrain Diffusion**: Only fine-tune the diffusion action head for UAV-specific control\n",
    "3. **New Embodiment**: Use `EmbodimentTag.UAV_QUADCOPTER` for separate action head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example fine-tuning command (run in terminal)\n",
    "finetune_command = \"\"\"\n",
    "python scripts/gr00t_finetune.py \\\n",
    "    --model_path=\"nvidia/GR00T-N1.5-3B\" \\\n",
    "    --data_path=\"./demo_data/uav.Landing\" \\\n",
    "    --data_config=\"uav_quadcopter\" \\\n",
    "    --embodiment_tag=\"uav_quadcopter\" \\\n",
    "    --output_dir=\"./checkpoints/uav_quadcopter_finetune\" \\\n",
    "    --batch_size=4 \\\n",
    "    --learning_rate=1e-4 \\\n",
    "    --num_epochs=50 \\\n",
    "    --freeze_backbone=true \\\n",
    "    --freeze_language_model=true \\\n",
    "    --only_train_action_head=true\n",
    "\"\"\"\n",
    "\n",
    "print(\"Fine-tuning command for UAV:\")\n",
    "print(finetune_command)\n",
    "\n",
    "print(\"\\nKey training parameters:\")\n",
    "print(\"- freeze_backbone=true: Keep VLM visual encoder frozen\")\n",
    "print(\"- freeze_language_model=true: Keep language model frozen\")\n",
    "print(\"- only_train_action_head=true: Only train UAV-specific diffusion head\")\n",
    "print(\"- embodiment_tag=uav_quadcopter: Use new UAV embodiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6617173",
   "metadata": {},
   "source": [
    "## Step 7: Policy Inference for UAV\n",
    "\n",
    "Example of how to use the fine-tuned UAV model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece92910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference code (uncomment when model is trained)\n",
    "inference_example = \"\"\"\n",
    "from gr00t.model.policy import Gr00tPolicy\n",
    "from gr00t.data.embodiment_tags import EmbodimentTag\n",
    "\n",
    "# Load the fine-tuned UAV model\n",
    "uav_policy = Gr00tPolicy(\n",
    "    model_path=\"./checkpoints/uav_quadcopter_finetune\",\n",
    "    modality_config=modality_configs,\n",
    "    modality_transform=composed_transforms,\n",
    "    embodiment_tag=EmbodimentTag.UAV_QUADCOPTER,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# Get action for current observation\n",
    "# observation should contain:\n",
    "# - video.front_camera: front camera image\n",
    "# - video.gimbal_camera: gimbal camera image  \n",
    "# - state.position: [x, y, z]\n",
    "# - state.orientation: [roll, pitch, yaw]\n",
    "# - state.velocity: [vx, vy, vz]\n",
    "# - state.battery: battery_level\n",
    "# - state.gps: [lat, lon, alt]\n",
    "# - annotation.human.task_description: \"Land on the designated platform\"\n",
    "\n",
    "action_chunk = uav_policy.get_action(observation)\n",
    "\n",
    "# Extract UAV controls from action\n",
    "flight_control = action_chunk[\"action.flight_control\"]  # [throttle, roll, pitch, yaw]\n",
    "velocity_command = action_chunk[\"action.velocity_command\"]  # [vx, vy, vz]\n",
    "gimbal_control = action_chunk[\"action.gimbal\"]  # [gimbal_pitch, gimbal_yaw]\n",
    "\"\"\"\n",
    "\n",
    "print(\"UAV Inference Example:\")\n",
    "print(inference_example)\n",
    "\n",
    "print(\"\\nExpected UAV action outputs:\")\n",
    "print(\"- flight_control: [throttle, roll, pitch, yaw] - Main flight controls\")\n",
    "print(\"- velocity_command: [vx, vy, vz] - Velocity setpoints\")\n",
    "print(\"- gimbal: [gimbal_pitch, gimbal_yaw] - Camera gimbal controls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4c54f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to adapt the GR00T humanoid VLA model for UAV quadcopter control:\n",
    "\n",
    "### Key Adaptations:\n",
    "1. **New Embodiment Tag**: `EmbodimentTag.UAV_QUADCOPTER` for separate action head\n",
    "2. **UAV State Space**: 13D including position, orientation, velocity, battery, GPS\n",
    "3. **UAV Action Space**: 9D including flight controls, velocity commands, gimbal\n",
    "4. **Modality Configuration**: Custom state/action mapping for UAV telemetry\n",
    "5. **Transforms**: UAV-specific normalization and data processing\n",
    "\n",
    "### Training Strategy:\n",
    "- **Leverage VLM**: Keep visual language model frozen to retain visual understanding\n",
    "- **Retrain Diffusion**: Only fine-tune the action head for UAV-specific control patterns\n",
    "- **Embodiment-Specific**: Train separate action head while sharing visual/language representations\n",
    "\n",
    "### Next Steps:\n",
    "1. Prepare your UAV dataset in LeRobot format\n",
    "2. Copy the modality.json file to your dataset's meta/ directory\n",
    "3. Run the fine-tuning script with UAV-specific parameters\n",
    "4. Test the trained model on UAV control tasks\n",
    "\n",
    "The key insight is that aerial robotics can benefit from the same visual language understanding as ground robots, with only the action generation needing UAV-specific adaptation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
